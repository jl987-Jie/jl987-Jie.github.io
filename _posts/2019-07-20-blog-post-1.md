---
title: 'Policy Gradient'
date: 2019-07-20
permalink: /posts/2019/07/20/blog-post-1/
tags:
  - reinforcement-learning
---

### Notes from book "Reinforcement Learning - An Introduction" by Sutton and Barto

### Policy Gradient

\begin{equation}
V_\theta(s) \approx V^{\pi}(s)
\end{equation}

\begin{equation}
Q_\theta(s, a) \approx Q^{\pi}(s, a)
\end{equation}

Policy based RL: policy generated from value function using $\epsilon$-greedy.

Directly parameterize the policy:

\begin{equation}
\pi_{\theta} (s, a) = P\left[ a \mid s; \theta \right]
\end{equation}

The goal is to find a policy $\pi$ with the highest value function $V^{\pi}$.

Policy-based RL:

Pros:
1. better convergence
2. high dimensional
3. stochastic policies

Cons:
1. convergence to local minimum
2. inefficient and high variance

